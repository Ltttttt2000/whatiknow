基本框架：数据获取、特征提取、数据转换、模型训练、模型选择、模型评价
驱动：神经网络、数据挖掘
分类
```
监督学习：已有数据集+已知关系-->模型 任务驱动
无监督学习：统计手段，无标签数据发现潜在模式
强化学习：从反馈/错误中学习
半监督学习
主动学习
```
ML过程：dataset准备收集、特征选择、算法选择、参数和模型选择、训练、评估
数据集的准备
数据标注领域
## 监督学习
1. 回归：连续的
	1. 看效率：==决策树、线性回归==
	2. 看准确率：==随机森林、神经网络、XGBoot==
2. 分类：离散的
	1. 看效率：==决策树、逻辑回归（LR）、朴素贝叶斯、支持向量机（SVM）==
	2. 看准确率：==核支持向量机、随机森林、神经网络、XGBoot==

流程：
1. 选合适目标任务的模型
2. 给一部分已知的问题和答案（训练集）训练
3. 规律总结形成方法论
4. 将测试集通过方法论解答

如何确定合适的模型
1. 数据类型：离散/连续
2. 数据维度大小
3. 数据量多少
4. 对模型准确性和效率的要求

### 线性回归 Linear Regression (LR)
$$
\begin{aligned}
Y &= Ax+B\\
Y &= a_{0}+a_{1}x_{1}+a_{2}x_{2}+···+\epsilon
\end{aligned}
$$
$A$ 是斜率slope, $B$是截距intercept
损失函数$MSE=\sum(Y_{i}-\overline{Y_{i}})^2$
梯度下降等优化算法，使loss function最小（先输入特征进行标准化/归一化，使各特征在相同尺度）
模型评估：RMSE，R-Squared


### 决策树 Decision Tree (DT)
回归和分类都行
根据条件进行选择的过程，每层节点通过某种规则分裂成多个节点，终端的叶节点即为分类结果。
构建决策树的目的：层数变高，使熵快速降低
熵降低的速度越快，DT的分类效率越高
熵Entropy：一个系统内在的混乱程度，样本种类的丰富性，完全同一类Entropy = 0
基尼指数：选择划分Gini最小的属性CART
特征选择也即选择最优划分属性，从当前数据的特征中选择一个特征作为当前节点的划分标准。我们希望在不断划分的过程中，决策树的分支节点所包含的样本尽可能属于同一类，即节点的 “纯度” 越来越高。而选择最优划分特征的标准不同，也导致了决策树算法的不同。

$$
\begin{aligned}
熵 Entropy(D)&=-\sum p_{k}log_{2}p_{k}\\
信息增益 Gain(D,a)&=Entropy(D)-\sum \frac{\lvert{D^v}\rvert}{\lvert D\rvert}Entropy(D^v)\\
增益率 Gain_ratio &=\\
基尼指数 Gini(D)&=1-\sum P_{k}^2
\end{aligned}
$$

决策树构建过程
Step 1：将所有特征看成一个个的节点
Step 2：将特征按不同属性分割，求出不同特征的信息增益/Gini指数等，不同的树取不同的值
Step 3：使用第二步遍历所有特征，选择出最优的特征，以及该特征的最优的划分方式，得出最终的子节点
Step 4：对子节点分别继续执行Step2-3，知道最终的子节点足够“纯”
如果下次该节点选出来的那一个属性是刚刚父节点分裂时用过的属性，则该节点已经达到了叶子节点，无需继续分裂。


防止过拟合
```
1. 预剪枝pre-pruning：树达到某一深度停止训练，测试时间⬇️，训练时间⬇️，过拟合风险⬇️，欠拟合风险⬆️，泛化性能较差
2. 后剪枝post-pruning：先找到树，再根据一定条件如限制叶子节点个数去掉分支，测试时间⬇️，训练时间⬆️，过拟合风险⬇️，欠拟合风险基本不变，泛化性能较好
3. 分支停止法
```
基本流程：递归过程


决策树类别：ID3、C4.5、CART
```
ID3算法：
C4.5算法：只能用于分类，不能进行特征组合。信息增益
CART算法：每个节点只能分成两个子节点，支持特征组合，可用于分类和回归。基尼指数Gini(D)= 1-SUM(p^2) 选择划分Gini最小的属性
```

优点
1.  速度快：计算量小，容易转换成分类规则，很少的数据准备
2. 准确性高
3. 可以处理连续和种类字段
4. 天然的可解释性，不需要任何领域知识和参数假设
5. 适合高维度数据
缺点
1. 对于各类痒不数量不一致的数据，信息增益偏向于那些更多数值的特征，不稳定，对训练样本敏感
2. 若完美分类容易过拟合
3. 忽略属性之间的相关性



### 随机森林
基于DT的最强大的算法，很多决策树的集成学习
从训练集有放回的选样本boostrap，随机选取部分特征
不知道异常样本和影响大的特征，降低影响
输出结果由投票决定（看大多数DT的分类结果）

```
优点：
1. DT独立训练，时间花费小
2. 不易过拟合
3. 能处理特征较多的高维数据，不用特征选择
4. 准确性高
```

### 集成学习Ensemble Learning
分类：
1. Bagging：个体不依赖，并行方法
2. Boosting：个体ML间强行依赖，串行
3. Stacking
#### Bagging
基于Bootstrap方法，对多个弱分类器投票、均值得到最终分类。bootstrap aggregating 有放回的抽样，为得到统计量的分布及置信区间，在N个数据集上训练N个模型
Bootstrap：有放回抽样得到等样本长的Bootstrap数据集，进行N次，对每个数据集训练弱分类器。
每个训练集互不相关，预测函数均匀平等
并行方法：Bagging-随机森林，行采样得到Bootstrap数据集，列采样随机选择M个特征，最终N颗决策树投出分类。
#### Boosting 提升方法
减小偏差，训练集在上一轮结果上调整，串行，预测函数加权
串行方法：AdaBoost-梯度提升决策树：GBDT，原始数据训练得到弱分类器，分类错误的样本提高权重，继续训练。
GBDT：Gradient Boost Decision Tree 梯度提升树
```
每棵树都建立在前棵树基础上，不断减少上一次残差A-->A+
应用：搜索、广告、推荐系统
优点：能处理数据标签，数值各类数据，解释性强
缺点：强依赖，训练时间长
```
XGBoost
```
相对于GBDT，目标函数=损失函数+正则项
损失函数：拟合数据的程度，有二阶导数看趋势，拟合更快，精度更高
正则项：一种惩罚机制，控制模型复杂度，叶子增多，模型复杂，时间上升，更容易overfitting
优点
1. 减少数据计算量，训练前先特征值排序
2. 捕捉依赖关系
3. 支持多种系统语言
缺点：高维稀疏特征，不实用小规模数据
```

AdaBoost：每次训练对失败的赋予大的权重

#### Stacking
训练一个模型用于组合其他各个模型

| Bagging | Boosting |
| ---- | ---- |
| 每个训练集互不相关 | 训练集在上一轮结果上调整，串行 |
| 预测函数均匀平等 | 预测函数加权 |



## 分类
### 逻辑回归 Logistic Regression （LR）
多用于二分类任务，本质：假设数据服从分布，用极大似然做参数估计
线性回归套用Sigmoid，变为0-1间表达概率
评估训练进度Cross-Entropy Loss分布间距离
$$
L=-\sum (y_{true}log(p)+(1-y_{true})log(p))
$$
应用：计算事件概率，股票
优点：
1. 减小极端值的影响，准确度上升
2. 可获得概率
缺点：只适合线性分布

### 朴素贝叶斯
关键词出现的概率（分别），特征维度相互独立
P(类别)P(关键词：出现的｜类别)
解决=0的情况：拉普拉斯平滑，关键词上人为加上一个出现的次数
优点：
1. 算法逻辑简单，易于实现
2. 分类过程中开销小
缺点：属性间关联大时效果不好

### 支持向量机 Support Vector Machines (SVM)
用于二分类
找到一个超平面，让所有样本的可信程度最高，距离代表分类的可信程度，越远越高
有交叉soft margin
不能用直线，映射到可用直线的区间（核）
优点：对样本依赖小，不易过拟合，小样本也行

## 无监督学习
无明确目的，不用打标签，无法量化效果
1. 聚类：K-means, DBSCAN
	1. 基于划分：K-means, bi-kmeans
	2. 基于密度：DBSCAN, Mean Shift, OPTICS
	3. 基于层次：Diana, agnes, HDBSCAN, Agglomerative
	4. 基于图：Chinese Whisper, CDP
2. 降维（减少变量）：PCA主成成分分析，SVD奇异值分解

### K-means
K是样本类别数
KNN：利用周围的认出新的类别，SVM：判断类别
Steps
```
1. 随机选K个样本作为分类基准，计算比较其他样本与他们的距离，离哪个近归于哪类
2. 找到每个类别的中心，迭代，计算所有样本与中心点距离按距离远近再分类
3. 找新的中心，计算距离，重新分类
4. 重复，直到结果与上次相同
```
优点：可解释性好，实现简单；不用过多数据信息；分类效果好
缺点：准确度不如监督学习；对K值的选择敏感；不能保证全局最优
K过大会对异常值敏感，K过小易过拟合

### DBSCAN基于密度的噪声应用空间
Eps:定义密度时的邻域半径；MmPts：定义核心点的阈值
优点：
1. 可对任意形状的稠密数据聚类，K-means只适用于凸集
2. 同聚类同时发现异常点，对异常值不敏感
3. 结果不偏不倚（相对K-means初始值影响大）
缺点：
1. 密度不均匀样本间距较大时质量差
2. 大样本收敛时间长
3. 要调参，参数组合影响大

### PCA：Principal Component Analysis
数据变量间存在相关性增加了分析难度，通过正交变换+少量线性无关变量代替相关变量，并保留大部分信息，根据新变量正交变换的方差将新变量成为：第一主成分
优点：
1. 主成分相交，消除原始数据间相互影响因素
2. 计算简单易实现
3. 大部分信息保留
缺点：
1. 模糊性，解释性差
2. 方差小不等于特征不重要，会导致损失信息

### SVD：Singular Value Decomposistion
优点：并行化；简单
缺点：解释性差
用于信息检索

### LDA线性判别分析 Fisher监督
### 深度学习 SparseAutoEncoder 

## 强化学习
一个**Agent**如何在**环境**下极大化奖励，通过感知**State**对**Action**的**Reward**反应，获得return。
偏重智能体与环境的交互，试错，从错误中学习
特点：试错学习来获得最佳策略；延迟回报，指导信息往往最后一个事件才给出
应用：机器人控制、用户交互、游戏、资源调度、推荐算法
分类：
- 根据环境是否已知：免模型学习model-free(不模拟环境，只根据反馈数据构建关于回报模型)、有模型学习model-based
- 根据学习方式：在线策略on-policy、离线策略off-policy
- 根据学习目标：基于策略policy-based、基于价值valued-based

## 进化学习
适应度（当前解答度的好坏的评分）


## 评估方法
混淆矩阵

| 左边真实右边预测 | Negative | Positive |
| ---- | ---- | ---- |
| True | TN | TP |
| False | FN | FP |
 $$
 \begin{aligned}
 Accuracy &= \frac{TN+TP}{All} 类别比例不均衡时影响效果\\ 
 Precision &= \frac{TP}{TP+FP} 识别为正中识别对的比例\\
 Recall &= \frac{TP}{TP+FN} 实际正中识别正确的\\
 F1_score &= \frac{2*Precision*Recall}{Precision+Recall}
 \end{aligned}
$$

PR曲线：取不同分类阈值
曲线下方面积越大AUC，性能越好
# 考题
机器学习特征选择的方法
```
DF文档频率：MI互信息法
IG信息增益：CHI卡方检验法
WLLR加权对数似然：WFO加权频率和可能性
```
评估指标
```
分类：准确率、混淆矩阵、ROC-AUC、P-R曲线
回归：MAE，MSE，RMSE，R-Sauqre
排序：AUC，MAP，NDCG，IDGC，MRR
聚类：轮廓系数，兰德指数，互信息
```
损失函数
```
分类：0-1loss, 交叉熵损失，对数，指数，合页
回归：MAE，MSE，Huber loss
其他：Focal loss
```

如何选择模型参数：交叉验证
交叉验证：选取一定比例作为验证机，不会参与模型训练，降低准准确性。通常采用K折交叉验证的方法，随后将全体样本分为K部分（3-20），每次用其中一部分作为验证集，重复K次，直到所有部分都被验证过。