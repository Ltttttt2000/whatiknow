# 一、基础定义和应用
深度学习是一种机器学习的分支，它着重于使用**深层神经网络**来模拟和解决复杂问题。深度学习的目标是通过多层次的非线性变换来学习数据的表征，以便实现对复杂模式的识别和建模。训练多层网络结构对未知数据进行分类或回归。
**分类：**
- 有监督学习（深度前馈网络、CNN、RNN）
- 无监督学习（深度信念网、深度玻尔兹曼机、深度自编码器）
**应用：**
- CV：图像分类、物体检测、图像分割、图像回归（预测坐标） 
- NLP：预测下文、情感分析、翻译、自动摘要等
- 语音识别、声纹识别、语音合成
- 综合：图像描述、图像生成、可视问答、视频生成
DL：构建多层网络对未知数据分类或回归
- 有监督：深度前馈网络，CNN，RNN
- 无监督：深度信念网，深度玻尔兹曼机，深度自编码器
应用
- CV：图像分类、物体检测、图像分割、回归
- 语音：语音识别ROC、声纹识别、语音合成
- NLP：语言模型、情感分析、翻译、摘要、推理
- 综合：图像描述、可视问答、图像生成、视频生成
感知机：神经网络计算FP forward propagtion逐层计算出结果；BP：由深到浅更新网络参数。
```
基本步骤
1. 前向传播：通过输入数据，按照网络的结构逐层计算每个神经元的输出。这个过程从输入层一直传递到输出层，形成模型的预测结果。
2. 计算损失：将模型的预测结果与实际标签进行比较，计算损失函数的值。损失函数衡量模型的性能，即模型对于给定输入的预测与实际标签之间的差异。
3. 反向传播：从输出层开始，计算损失函数对每个参数的偏导数（梯度）。这是通过使用链式法则来计算每一层的梯度，然后将梯度传播回网络的每一层。
4. 参数更新：使用梯度下降等优化算法，根据梯度的反方向更新每个参数，以降低损失函数的值。更新规则通常如下： 
5. 新参数=旧参数−学习率×梯度新参数=旧参数−学习率×梯度
6. 重复训练：重复步骤1到步骤4，直到损失函数收敛到满意的程度，或达到预定的迭代次数。

关键概念
1. 链式法则（Chain Rule）反向传播的核心是链式法则，它允许计算复合函数的导数。在神经网络中，每一层都是一个函数，通过链式法则计算损失函数对于每个参数的导数。
2. 局部梯度：反向传播中计算的梯度是损失函数对于每个参数的局部梯度，表示参数的微小变化对损失的影响。
3. 参数共享：反向传播允许神经网络中的参数共享学习，即同一层的多个神经元共享相同的权重。
```
## **神经网络:**
神经网络Neural Networks是一种模拟生物神经网络结构和功能的进行**分布式并行信息处理**的算法数学模型。它由神经元（或节点）和它们之间的连接组成，这些连接具有权重，可以通过学习调整。
神经网络可用于解决各种问题，包括分类、回归和聚类等。神经网络通常包括输入层、隐藏层和输出层。每个神经元都与下一层的神经元相连接，并且每个连接都有一个权重，用于传递信息。

神经网络的计算主要有两种：
1. 前向传播（foward propagation, FP）作用于每一层的输入，通过逐层计算得到输出结果
2. 反向传播（backward propagation, BP）作用于网络的输出，通过计算梯度（梯度下降）由深到浅更新网络参数。反向传播算法（Backpropagation）是一种用于训练神经网络的监督学习算法。它通过计算损失函数对每个参数的梯度，并反向传播这些梯度来更新网络的权重，从而最小化或最大化损失函数。反向传播是多层感知机（Multilayer Perceptron，MLP）等深度学习模型中最常用的优化算法之一。
分类：
前馈神经网络（Feedforward Neural Networks，FNN）和反馈神经网络（Recurrent Neural Networks，RNN）是两种不同类型的神经网络，其主要区别在于信息的传递方式和网络结构。主要区别在于信息的传递方式。FNN 是单向传递的，适用于静态数据的建模。而RNN 允许信息在网络中形成循环，适用于处理具有时间关系的序列数据。近年来，为了解决 RNN 中的梯度消失等问题，出现了一些改进的结构，如长短时记忆网络（LSTM）和门控循环单元（GRU）。
1. **前馈神经网络（FNN）：**
```
在前馈神经网络中，信息沿着一个方向传递，从输入层经过隐藏层到输出层，没有形成循环或反馈。FNN 是一种层与层之间完全连接的网络，每一层的神经元与下一层的所有神经元相连接。每个连接都有一个权重，这些权重用于调整信息的传递。
FNN 主要用于静态数据的建模和处理，如图像分类、语音识别等。
```
3.  **反馈神经网络（RNN）：**
```
反馈神经网络允许信息在网络中形成循环，以便神经元之间的连接可以创建时间上的依赖关系。这使得网络能够处理序列数据和动态模式。包含循环连接，允许网络在同一层内的神经元之间建立连接。这种连接允许网络在处理序列数据时记住先前的信息。
RNN 在处理时间序列数据（如自然语言处理、语音识别、股票预测等）时表现出色，因为它们能够捕捉时间依赖关系。
```

## 超参数
在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。
 通常存在于
```
1.  定义关于模型的更高层次的概念，如复杂性或学习能力。
2.  不能直接从标准模型培训过程中的数据中学习，需要预先定义。
3.  可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定
```
超参数具体来讲比如算法中的学习率（learning rate）、梯度下降法迭代的数量（iterations）、隐藏层数目（hidden layers）、隐藏层单元数目、激活函数（ activation function）都需要根据实际情况来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。

**如何寻找超参数的最优值？**
​在使用机器学习算法时，总有一些难调的超参数。例如权重衰减大小，高斯核宽度等等。这些参数需要人为设置，设置的值对结果产生较大影响。常见设置超参数的方法有：
```
1. 猜测和检查：根据经验或直觉，选择参数，一直迭代。
2. 网格搜索：让计算机尝试在一定范围内均匀分布的一组值。
3. 随机搜索：让计算机随机挑选一组值。
4. 贝叶斯优化：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。
5. MITIE方法，好初始猜测的前提下进行局部优化。它使用BOBYQA算法，并有一个精心选择的起始点。由于BOBYQA只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在MITIE的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。稍后我会再讨论这一点。
6. 最新提出的LIPO的全局优化方法。这个方法没有参数，而且经验证比随机搜索方法好。
```

**超参数搜索一般过程**
```
1. 将数据集划分成训练集、验证集及测试集。
2. 在训练集上根据模型的性能指标对模型参数进行优化。
3. 在验证集上根据模型的性能指标对模型的超参数进行搜索。
4. 步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣。
其中，搜索过程需要搜索算法，一般有：网格搜索、随机搜过、启发式智能搜索、贝叶斯搜索
```


# 三、深度学习常用模型

神经网络模型：感知机、CNN、RNN、LSTM、GRU、GAN等。现在在应用领域应用的做多的是DNN，CNN和RNN。
### 3.1  感知机
1957年，由Rosenblatt提出会，是**神经网络和支持向量机**的基础。复杂一些的感知机由简单的感知机单元组合而成。
多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络。相比于单独的感知机，多层感知机的第i ii层的每个神经元和第i − 1 i-1i−1层的每个神经元都有连接。
**多层感知机（Multilayer Perceptron，MLP）：** 是最基本的前馈神经网络，由多个全连接层组成，用于解决分类和回归问题。
### DNN
DNN是传统的全连接网络，可以用于广告点击率预估，推荐等。其使用embedding的方式将很多离散的特征编码到神经网络中，可以很大的提升结果。
### CNN
卷积神经网络（Convolutional Neural Networks，CNNs）： 主要应用于CV领域，通过卷积层、池化层等结构来有效地捕捉图像中的局部特征。  CNN的出现主要解决了DNN在图像领域中参数过多的问题。同时，CNN特有的卷积、池化、batch normalization、Inception、ResNet、DeepNet等一系列的发展也使得在分类、物体检测、人脸识别、图像分割等众多领域有了长足的进步。同时，CNN不仅在图像上应用很多，在自然语言处理上也颇有进展，现在已经有基于CNN的语言模型能够达到比LSTM更好的效果。在最新的AlphaZero中，CNN中的ResNet也是两种基本算法之一。

**结构**
```
1. 卷积层（Convolutional Layer）: 
提取输入数据的局部特征。卷积操作通过在输入数据上滑动卷积核（filter）来检测图像中的模式和特征。这有助于网络捕获图像的空间层次结构。卷积操作在不同位置共享参数，使得网络能够检测到图像中相同的模式，无论其位置在何处。 滑动的步幅(stride)决定了卷积核在图像上的移动距离，影响了输出特征图的尺寸。

2. 激活函数(Activation Function):
在卷积操作后，通常应用一个非线性激活函数（如ReLU），以引入非线性关系，增加网络的表达能力。

3. 池化层（Pooling Layer）：
下采样：通过降低特征图的尺寸，减少计算复杂度，同时保留关键信息。池化层最大池化是常见的一种操作，选择每个区域中的最大值作为新的特征值。

4. 多层堆叠
通常由多个卷积层和池化层交替堆叠而成，逐渐提取更高级别的特征。这使得网络能够层次化地学习图像的抽象表示。

5. 全连接层（Fully Connected Layer）：
在卷积层和池化层之后，通常会添加一个或多个全连接层，用于对从先前层提取的特征进行组合和分类。

6. Softmax激活函数
对于分类任务，通常在最后的全连接层之后应用Softmax激活函数，将输出转换为每个类别的概率分布。

7. 反向传播和优化：
使用反向传播算法来计算损失函数对网络参数的梯度，然后通过梯度下降等优化算法来更新参数，使得网络的预测逐渐接近真实标签。
```
**应用**
```
1. 图像分类
2. 物体检测：通过在图像中滑动卷积窗口，检测和定位图像中的物体。一些流行的物体检测框架，如YOLO（You Only Look Once）和Faster R-CNN，就是基于CNNs设计的。
3. 语义分割：将图像中的每个像素标记为属于某个特定类别。这对于理解图像中的对象和场景非常重要。
4. 人脸识别：能够从图像中学习和提取面部特征。
```

### RNN 
循环神经网络（Recurrent Neural Networks，RNNs）专注于处理序列数据依赖关系，如自然语言处理任务、时间序列分析等。
由于梯度消失和爆炸问题，常见的改进型结构包括长短时记忆网络（LSTM）和门控循环单元（GRU）。
**应用**
```
1. 序列建模：
RNNs在处理序列数据上表现出色，如文本、语音、音乐等。它们能够捕捉序列中的时间依赖关系，对前后关联的信息进行建模。在自然语言处理中，RNNs被广泛用于语言建模、机器翻译、命名实体识别等任务。
2. 时间序列分析：
适用于处理时间序列数据，如股票价格、气象数据、生物信号等。通过学习时间序列中的模式，RNNs可以进行预测、异常检测等应用，对于具有时序特征的数据分析具有很强的能力。
3. 语音识别：
处理变长的语音信号序列，有效地捕捉语音信号中的语法和语义信息。这使得RNNs在语音识别应用中表现出色。
4. 手写体识别：
能够识别和生成手写体字符，理解笔画的顺序和结构。
5. 图像描述生成：
在CV领域，RNNs与CNN结合使用，用于生成图像描述。通过接收图像的特征表示，RNNs能够生成与图像内容相关的自然语言描述。
6. 生成模型：
如文本生成、音乐生成等。通过学习训练数据的统计规律，RNNs可以生成具有类似特征的新序列数据。
```
**优点**
在自然语言处理（NLP）等领域中使用循环神经网络（Recurrent Neural Networks，RNNs）的主要原因是处理序列数据的能力。NLP任务中的文本数据通常是序列化的，即一系列词语或字符按照顺序排列。RNNs在处理这样的序列数据时具有以下优势：
```
1. 处理变长序列：能够处理变长的序列，因为它们在每个时间步都接收一个输入并产生一个输出。这种灵活性使得RNNs非常适用于处理不同长度的文本。
2. 上下文理解：通过记忆前面的信息，能够建模上下文之间的关系。这对于理解句子或文本中的语境非常重要，例如在机器翻译中，理解前文可以帮助正确翻译后文。
3. 时序信息：保留了时序信息，能够捕捉单词或字符在序列中的顺序关系。这对于任务如语言建模、文本生成以及时间序列分析等有关键作用。
4. 共享参数：相同的权重被用于处理不同时间步上的输入，这实现了参数的共享，减少了需要学习的参数数量，提高了模型的效率。
```
**问题**：梯度消失
```
在反向传播过程中，由于多次相乘导致的梯度逐渐变得非常小，最终趋近于零。这会导致网络无法有效地学习或传播远距离依赖关系，因为梯度信息在传播过程中变得非常微弱，无法对远处的权重进行有效的更新。

梯度消失问题的产生主要原因是在反向传播过程中，梯度是通过对时间步的多次连续相乘来传播的。在这个过程中，如果涉及的权重都小于1，那么梯度就会变得越来越小，最终可能消失到接近零的程度。这尤其在长序列上表现得更为明显。
```
### LSTM
**长短时记忆网络（Long Short-Term Memory，LSTM）：** 是一种专门设计用于解决梯度消失问题的RNN结构，能够更好地捕捉长期依赖关系。
引入了一个特殊的记忆单元，可以在长序列中有效地保留和更新信息。这通过三个门来实现：
1. 遗忘门（Forget Gate）:决定要从记忆中删除哪些信息。
2. 输入门（Input Gate）:决定要添加哪些新信息到记忆中。
3. 输出门（Output Gate）:决定记忆中的哪些信息将被传递给下一个时间步。
### GRU
门控循环单元（Gated Recurrent Unit，GRU）是对LSTM的一种简化版本，采用了更少的参数，同时在计算上更为轻便。
1. 更新门（Update Gate）:决定新的输入信息有多少要被加入到当前的记忆中。
2. 重置门（Reset Gate）:决定要忽略多少旧的记忆。
与LSTM比较
```
共同点：
- 都引入了门控机制，允许网络选择性地忽略或更新记忆，从而提高对长序列的建模能力。
- 这两者都在处理序列数据时能够有效地捕捉长期依赖关系，相对于传统的RNNs具有更好的性能。

选择：
在实际应用中，LSTMs和GRUs的性能可能取决于具体任务，数据集的特征，以及可用的计算资源。通常来说，LSTMs在某些任务上表现更好，但GRUs更简单，更经济，有时也能表现出色。选择使用哪种结构通常取决于实际情况和需求。
```
### GAN
**生成对抗网络（Generative Adversarial Networks，GANs）：** GAN在图像生成、图像编辑、超分辨率、风格转换、生成艺术等领域都有广泛的应用。
**GAN基本原理**
```
GAN由生成器和判别器组成，生成器试图生成逼真的样本，而判别器试图区分真实样本和生成样本。生成器和判别器通过对抗训练相互优化。
```
**解释GAN的训练过程。**
```
生成器生成样本，判别器评估样本真实性。损失函数包括生成器的生成损失和判别器的判别损失。训练过程是通过优化生成器和判别器的参数，使生成器生成更逼真的样本，判别器更准确地识别真实和生成样本。
```
**什么是生成器和判别器的损失函数？**
```
生成器的损失函数通常包括生成样本与真实样本的差异，而判别器的损失函数包括对真实样本和生成样本的判断准确性。
```
**GAN的训练中可能遇到的问题是什么？**
```
常见的问题包括模式崩溃、训练不稳定、生成器和判别器的失衡等。这可能需要通过调整模型架构、超参数、损失函数等来解决。
```
 **解释GAN中的模式崩溃是什么？**
```
模式崩溃是指生成器倾向于生成相似的样本，而不是多样化的样本。这可能导致生成器只覆盖了训练数据的一小部分模式。
```
**什么是条件生成对抗网络（cGAN）？有什么优势？**
```
cGAN在生成过程中引入条件信息，即在生成器和判别器中加入条件向量。这使得生成的样本能够根据给定的条件进行生成，扩展了GAN的应用范围。
```
**GAN和VAE（变分自动编码器）之间有什么区别？**
```
GAN强调生成逼真的样本，而VAE强调学习数据分布的潜在结构。GAN的生成是通过对抗训练得到的，而VAE通过最大化似然性进行训练。
```
**如何评估生成对抗网络的性能？**
```
性能评估可以使用生成样本的质量、多样性、覆盖率等指标，以及判别器的准确性。另外，也可以使用人工评估或使用特定任务的性能度量来评估。
```
纳什均衡
### Autoencoders
**自动编码器（Autoencoders）：** 通过学习如何表示输入数据的压缩表示，包括变分自动编码器（Variational Autoencoders，VAEs）等。

### 转移学习模型
**转移学习模型：** 包括预训练的模型，如BERT（用于自然语言处理）和ImageNet上预训练的卷积神经网络等，通过迁移学习在新任务上取得优异性能。

### 深度强化学习模型
**深度强化学习模型：** 包括深度Q网络（Deep Q Networks，DQN）、深度确定性策略梯度（Deep Deterministic Policy Gradient，DDPG）等，用于处理强化学习问题。

### Attention
**注意力模型：** 包括Transformer模型，用于处理序列数据，广泛应用于自然语言处理任务，如机器翻译（Transformer的子模型）。


# 四、深度学习框架
1. **TensorFlow：** 由Google开发的开源深度学习框架，具有灵活性和广泛的社区支持。它支持动态图和静态图，广泛应用于各种深度学习任务。
2. **PyTorch：** 由Facebook开发的深度学习框架，以简洁的API和易用性而闻名。它采用动态图模型，使得模型定义和调试更为直观。
3. **Keras：** 最初由François Chollet开发，现已成为TensorFlow的高级API。Keras提供简单的接口，使得构建神经网络变得更加容易。
4. **Caffe：** 由伯克利视觉与学习中心（BVLC）开发的深度学习框架，主要用于图像分类和卷积神经网络。Caffe的速度较快，适用于实时应用。
5. **MXNet：** Apache软件基金会支持的深度学习框架，具有良好的分布式计算支持。MXNet支持动态图和静态图，并提供Gluon API以简化模型定义。
6. **Chainer：** 由日本Preferred Networks公司开发的深度学习框架，支持动态图模型。它被设计为直观和易于使用。
7. **Theano：** 由蒙特利尔大学的深度学习研究组开发的深度学习框架，具有高效的数学表达式编译。然而，开发已经停止，推荐使用更先进的框架。
8. **CNTK（Microsoft Cognitive Toolkit）：** 由Microsoft开发的深度学习框架，支持动态图和静态图。CNTK在性能上具有一定优势。
9. **DL4J（Deeplearning4j）：** 由Skymind公司开发的Java深度学习框架，适用于Java和Scala开发者。它具有与Hadoop和Spark的集成支持。
10. **PaddlePaddle：** 由百度开发的深度学习框架，具有飞桨领域特定处理器（PaddlePaddle DNN）的支持，适用于各种深度学习应用。
### 4.1 PyTorch
PyTorch介绍
```
PyTorch是一个开源的深度学习框架，由Facebook开发，具有动态计算图的特性，适用于构建和训练各种深度学习模型。
```
**解释动态计算图和静态计算图的区别。**
```
动态计算图是在运行时构建的，允许灵活地调整图的结构。
而静态计算图在运行之前就已经定义好，通常需要整个图的结构在编译时就确定。
```
**PyTorch中张量（Tensor）是什么？**
```
张量是PyTorch中表示多维数据的基本数据结构，类似于NumPy数组。PyTorch的张量支持GPU加速，提供了各种操作和函数用于数学运算。
```
**如何在PyTorch中定义神经网络模型？**
```
通过定义一个继承自`torch.nn.Module`的类，然后在类的构造函数中定义网络层和操作。最后，通过实现`forward`方法来定义数据在模型中的前向传播过程。
```
**什么是PyTorch中的Autograd？**
```
Autograd是PyTorch中的自动求导机制，它能够自动计算张量上的梯度。这使得在构建和训练神经网络时，无需手动计算梯度。
```
**解释PyTorch中的DataLoader的作用。**
```
DataLoader用于加载和迭代训练集和测试集数据，同时提供了数据的批处理、随机化和并行加载等功能，使得数据输入模型变得更加方便。
```
**如何在PyTorch中实现模型参数的共享？**
```
通过在定义模型时使用相同的`torch.nn.Module`实例来共享模型参数。
```
**什么是PyTorch中的优化器？**
```
优化器是用于更新模型参数的算法，例如随机梯度下降（SGD）、Adam、RMSprop等。在PyTorch中，`torch.optim`模块提供了各种优化器的实现。
```
**如何在PyTorch中保存和加载模型？**
```
使用`torch.save`保存模型的参数，并使用`torch.load`加载参数。也可以使用`state_dict`保存和加载模型的状态字典。
```
**解释PyTorch中的CUDA和如何将模型迁移到GPU上。**
```
CUDA是NVIDIA提供的并行计算平台，PyTorch可以利用CUDA实现GPU加速。可以通过将模型和张量移动到GPU上，使用`.to('cuda')`方法来实现模型在GPU上的运算。
```

# 五、 深度学习八股
$outputsize = \frac{inputsize+2*padding-kernel}{stride+1}$
卷积向下取整，池化向上取整



**过拟合Overfitting**
```
1. 概念：将抽样误差也很好的拟合
模型在训练数据上表现良好，但在未见过的测试数据上表现较差的情况。过拟合发生时，模型学习了训练数据中的噪声和细节，而没有泛化到更广泛的数据集上。这可能导致模型对训练数据过于敏感，无法正确地推广到新的、未见过的数据。

2. 原因：
样本选取有误（数据少，无法描述问题的真实分布，数据有噪声）
噪声干扰过大
模型训练过度
模型复杂度高：模型具有太多参数，以适应训练数据中的每一个细节，甚至包括噪声。
训练数据不足：如果训练数据量太小，模型可能无法捕捉数据的真实分布，从而过度依赖有限的样本。

3. 解决
（1） 增加训练数据：提供更多的样本可以帮助模型更好地理解数据的分布
（2） 正则化：正则化通过在模型的损失函数中引入额外的项，限制模型的复杂性，有助于缓解过拟合问题
（3） 早停Early Stopping：在训练过程中监控验证误差，一旦验证误差停止下降，即停止训练，避免模型过度拟合训练数据
（4） 交叉验证：将数据划分为多个子集，进行多次训练和验证，从而更好地评估模型的泛化性能
（5） 降低模型复杂度：减少隐藏层单元数，特征选择
（6） Dropout
（7） 集成学习Bagging, Boosting：使用集成学习方法，如随机森林、梯度提升等，通过组合多个模型的预测结果来提高泛化性能
（8） 数据增强 Date Augmentation： 在训练数据中引入一些变化，如旋转、缩放、平移等，以生成更多样本，有助于模型更好地学习数据的不变性
```

**欠拟合**
```
1. 表现的很差
2. 原因：模型的训练太简单，训练特征过少，还未停止收敛就停止循环
3.  解决：增加样本数量；增加模型参数，提高模型复杂度；增加循环次数；查看是否学习率过高导致无法收敛
```

high bias：欠拟合
high variance：过拟合
偏差：预测值与真实值的关系
方差：衡量预测值之间的关系，离散程度
误差=偏差+方差

**正则化**
L1正则化和L2正则化是两种用于防止过拟合的正则化技术，它们在损失函数中引入额外的项，以惩罚模型参数的大小。这有助于使模型更加简单，减少对训练数据的过度拟合。
```
L1正则化是指在损失函数中添加参数权重的绝对值之和，通常乘以一个正则化参数（λ）。倾向于使部分参数为零，从而实现特征的稀疏性。这意味着在训练过程中，L1正则化可以促使模型对不相关或不重要的特征进行选择，从而减少参数的数量。
L2正则化是指在损失函数中添加参数权重的平方和的一半，乘以一个正则化参数（λ）。通过对所有权重进行平方惩罚，倾向于使每个权重都变得很小，但不会完全为零。它有助于减小权重之间的差异，使得模型对特征的影响更为均衡。

稀疏性：L1正则化倾向于使权重变得稀疏，即某些权重为零，而L2正则化倾向于使权重更均匀，但不会使其为零。
特征选择：L1正则化常用于特征选择，因为它可以使模型更容易忽略对目标贡献较小的特征。
参数缩放：L2正则化通常比L1正则化对大权重的惩罚更大，因为它是对平方项的惩罚。
在实践中，通常采用L1和L2正则化的组合，称为弹性网络（Elastic Net），以综合利用两者的优势。
```


**激活函数**
```
引入非线性，增强神经网络表达能力，在最后一层时导出最后的结果
性质：可微性、单调性：保证单层网络是凸函数、 输出值的范围有限时更加稳定，输出值无限时训练更高效，但需要更小的学习率

常见激活函数
(1) Sigmoid：可能梯度消失，很好的解释性，计算型大
(2) ReLU：不会梯度弥散，计算更快，梯度下降时收敛快；ReLU死亡问题
(3) softmax：多分类多个类别间是互斥的
```



**优化器**

**梯度下降**
梯度下降算法是一种用于优化目标函数的迭代优化算法。它是机器学习和深度学习中最常用的优化算法之一，其目标是找到函数的局部最小值，从而最小化或最大化该函数的值。梯度下降的基本思想是通过不断迭代调整参数，使目标函数的值逐渐趋近最小值。
**梯度下降的工作原理：**
```
1. 初始化参数：选择一个初始的参数值，这可以是随机的或通过其他方式选择的初始值。
2. 计算梯度：计算目标函数对于当前参数的梯度（导数）。梯度是一个向量，指示了目标函数在当前点上升最快的方向。在梯度下降中，我们希望沿着梯度的负方向移动，以减小目标函数的值。
3. 更新参数：根据梯度的负方向更新参数，使得目标函数的值减小。更新规则通常为： 新参数=旧参数−学习率×梯度新参数=旧参数−学习率×梯度 其中，学习率是一个预先指定的超参数，控制每次迭代中参数的调整幅度。
4. 重复迭代：重复步骤2和步骤3，直到满足停止条件，例如达到最大迭代次数或梯度的大小变得很小。
```
**随机梯度下降（Stochastic Gradient Descent，SGD）**
在每一次迭代中，随机选择一个样本计算梯度，更新参数。SGD通常具有更快的收敛速度，但由于随机性，也更加不稳定。
**批量梯度下降（Batch Gradient Descent）**
在每一次迭代中，使用整个训练数据计算梯度。
**小批量梯度下降（Mini-Batch Gradient Descent）**
结合了批量梯度下降和随机梯度下降的优点，每次迭代使用一小部分（mini-batch）样本来计算梯度和更新参数。


## 梯度问题
**梯度下降的问题**
```
学习率选择：学习率过大可能导致算法发散，学习率过小可能导致收敛速度慢。
局部最小值：算法可能陷入局部最小值，而不是全局最小值。
鞍点问题：在高维空间中，梯度下降可能停滞在鞍点上。
```

**梯度消失**
```

1. 原因：由链式法则全连接导致（网络层太深），选择了不合适的损失函数
2. 梯度更新以指数形式衰减，损失函数为0，只等于后面几层网络
3. 解决方法
① pre-training + fine-tunning
② 梯度剪切：对梯度设置阈值（爆炸）
③ 权重正则化
④ ReLU等激活函数
⑤ batch normalization
⑥ 残差网络
```
**梯度爆炸**
```
梯度更新以指数形式增加，损失函数为NaN了，权重值变得非常大
```


**梯度弥散**

**标准化+归一化**
1．why：样本不同特征/属性所在数值范围差异大，导致训练不收敛或其他问题，所有数据在相同的取值空间更容易处理，方便模型的统一化和规范化，更容易发现数据的本质规律
区别
归一化主要是应用于没有距离计算的地方上
标准化则是使用在不关乎权重的地方上

2．归一化：[0,1]
（1）加快了收敛速度，可能提高精度

3．标准化：转换到0值的附近，需要均值和标准差



## 深度学习训练步骤
1. 明确目标
2. 划分数据集：将数据集划分为训练集、验证集和测试集。训练集用于模型的训练，验证集用于调整超参数，测试集用于评估模型的性能。
3. 选择评估指标：如准确度、精确度、召回率、F1分数等。评估指标应与任务目标一致。
4. 调参数
	1. 通常，可以从一个小的学习率开始，观察模型的训练进程和性能，逐渐调整学习率。学习率过大可能导致训练不稳定，学习率过小可能导致训练缓慢。
	2. 批量大小对训练的影响也很大。较小的批量大小可能导致模型更快地收敛，但噪声较大；较大的批量大小可能减小噪声，但可能导致训练不稳定。可以尝试不同的批量大小，找到合适的值。
	3. 优化算法也是一个关键的超参数。常见的优化算法包括随机梯度下降（SGD）、Adam、RMSprop等。不同的任务可能需要不同的优化算法。可以尝试不同的优化算法，并观察模型的收敛速度和性能。
	4. 正则化项如权重衰减（weight decay）、Dropout等有助于防止过拟合。可以调整正则化项的权重，观察对模型性能的影响。
	5. 可以尝试调整网络的深度、宽度、层次结构等。增加或减少层次、节点数，使用不同的激活函数，都可能影响模型性能。
	6. 对于某些任务，使用预训练模型作为迁移学习的起点可能有助于模型的快速收敛。可以选择预训练模型并微调参数。
	7. 通过交叉验证来评估不同超参数组合的性能，防止模型对特定验证集过拟合。
	8. 可以使用一些自动调参工具，如贝叶斯优化、网格搜索等，以加速调参过程。
5. 定期监控训练过程，包括损失的变化、训练集和验证集上的性能指标等。及时发现问题并进行调整。

如何判断模型是否收敛
模型的收敛通常通过监控训练过程中的指标来进行评估。以下是一些判断深度学习模型是否收敛的常见方法：
1. **损失函数下降：** 监控训练过程中的损失函数，观察其是否在逐步下降。损失函数的下降是模型在学习任务上逐渐优化的一个指标。
2. **验证集性能：** 使用验证集评估模型的性能，观察模型是否在验证集上表现稳定。当验证集上的性能不再提升或趋于平稳时，模型可能已经收敛。
3. **训练集和验证集的性能趋于一致：** 当模型开始收敛时，训练集和验证集上的性能差距通常会逐渐减小，最终趋于一致。
4. **学习曲线平稳：** 通过绘制学习曲线，观察训练和验证过程中的性能变化。在模型收敛时，学习曲线通常趋于平稳。
5. **梯度趋近零：** 监控模型参数的梯度。在模型收敛时，梯度通常会趋近零。如果梯度较小且稳定，可能表示模型已经收敛。
6. **早停法：** 使用早停法（early stopping）来防止模型过拟合。当验证集性能在一定轮次内不再提升时，停止训练，以避免过度拟合。
7. **稳定的学习率曲线：** 学习率曲线显示每个迭代步骤中学习率的变化。当学习率曲线趋于平稳时，可能表示模型已经收敛。
8. **生成样本质量：** 对于生成模型，可以通过观察生成样本的质量来判断模型是否收敛。随着训练的进行，生成样本应该变得更加逼真和多样化。
    

需要注意的是，模型的收敛并不总是一件容易的事情，因为它可能受到初始参数、数据质量、模型架构等多方面的影响。因此，在判断模型是否收敛时，通常需要综合考虑多个因素，并在不同的任务和数据集上进行验证。同时，一些噪声和波动在训练过程中是正常的，而不一定表示模型未收敛。

### 1. 如何选择合适的评估指标

- **分类任务：** 对于二分类问题，可以使用准确度（Accuracy）、精确度（Precision）、召回率（Recall）、F1分数等。对于多分类问题，可以考虑使用混淆矩阵、分类报告中的多个指标来全面评估模型性能。
- **回归任务：** 常用的指标包括均方误差（MSE）、平均绝对误差（MAE）、R平方等。这些指标用于度量模型对真实值的拟合程度。
- **生成模型：** 对于生成模型，评估指标可能包括生成样本的质量、多样性、覆盖率等。例如，在图像生成任务中，可以使用图像质量指标（如SSIM、PSNR）和人工评估等。
- **强化学习：** 对于强化学习任务，可能需要考虑累积奖励、策略的稳定性等指标。
    
### 2. 如何调整学习率

- **学习率衰减：** 使用学习率衰减策略，例如按指数衰减、按步骤衰减或按余弦衰减。这有助于在训练过程中逐渐减小学习率，使得模型更容易收敛。
- **网格搜索：** 尝试使用网格搜索或随机搜索的方法，搜索不同的学习率值，然后选择在验证集上性能最好的学习率。
- **自适应学习率算法：** 使用自适应学习率算法，如Adam、Adagrad、RMSprop等。这些算法通常能够根据梯度的信息自动调整学习率。
- **学习率范围测试：** 使用学习率范围测试，通过在一个较大的学习率范围内进行训练，找到一个适当的学习率范围。

### 3. 如何调整批量大小

- **小批量训练：** 在计算资源允许的情况下，尽可能选择较小的批量大小，这有助于模型更快地收敛。
- **尝试不同批量大小：** 尝试不同的批量大小，观察模型在验证集上的性能，选择在性能和计算资源之间取得平衡的批量大小。
- **动态调整批量大小：** 可以尝试使用动态批量大小，例如在训练初期使用较大的批量大小，然后逐渐减小，以加速模型的收敛。
    

### 4. 如何选择优化算法
- 基本梯度下降：SGD，BGD，MBGD，Adadelta，Adagrad，RMSprop
- 动量梯度下降
- Adam优化器

- **Adam算法：** Adam是一个常用的自适应学习率优化算法，通常在许多任务中表现良好。
- Adagrad：适合处理稀疏剃度，依赖人工设置的全局学习率
- Adadelta: 改进：梯度平方的指数加权代替全部平方和（解决后面趋近于0），更新量的指数加权平均-全局学习率，训练初中期很快，后期在局部最小值抖动。
- **SGD算法：** Stochastic Gradient Descent 随机梯度下降（SGD）虽然简单，但在某些情况下仍然有效。可以通过调整学习率和学习率衰减来优化SGD的性能。只使用耽搁训练样本，速度快，but目标函数值剧烈波动，局部or不收敛，整体效率不高。
- BGD：批量Batch，每次迭代更新所有样本，保证收敛到凸误差表面的全局最小值和局部最小值，数据量大速度慢。
- MBGD：小批量，比SGD收敛更稳定，比BGD快，通常样本数固定。2^N
- **RMSprop算法：** RMSprop是另一种自适应学习率算法，尤其在处理非平稳目标时效果较好。Root Mean Square Prop均方根传递。处理非平稳目标，对RNN好。
- **Nadam算法：** Nadam是结合Nesterov动量和Adam的一种优化算法，对某些任务可能更有效。

### 5. 如何调整网络结构

- **深度和宽度：** 尝试增加或减少网络的深度和宽度，观察对性能的影响。更深的网络可能对复杂任务更有利。
- **卷积核大小：** 在卷积神经网络中，尝试不同大小的卷积核，找到最适合任务的核大小。
- **激活函数：** 尝试不同的激活函数，如ReLU、Leaky ReLU、ELU等，选择对模型性能有积极影响的激活函数。
- **正则化：** 添加正则化项，如Dropout、权重衰减等，以防止模型过拟合。

### 6. 交叉验证

交叉验证是评估模型性能和泛化能力的有效方法。以下是一些建议：

- **K折交叉验证：** 将数据集分成K个折叠，进行K次训练和验证，确保每个样本都在验证集中出现一次。
- **网格搜索和交叉验证结合：** 结合网格搜索和交叉验证，通过尝试不同的超参数组合来选择最佳模型。
- **Stratified K-Fold：** 对于不平衡数据集，使用Stratified K-Fold确保每个


# 基础知识
2. 激活函数
- 为什么引入非线性激励函数
深度学习的前提是神经网络的隐层加上了非线性激活函数，提升了模型的非线性表达能力，使得神经网络可以逼近任意复杂的函数。假如有一个100层的全连接神经网络，其隐层的激活函数都是线性的，则从输入层到输出层实际上可以用一层全连接来等价替换，这样就无法实现真正的深度学习。
引入非线性，更好的分类
- 激活函数Sigmoid, Tanh, ReLu 三个激活函数的缺点或不足
Sigmoid 函数：
```
梯度消失：Sigmoid 函数的梯度在输入很大或很小的时候接近于零，导致梯度消失问题。在深度神经网络中，这可能导致梯度无法有效地通过网络反向传播，影响模型的训练。
非零均值：Sigmoid 函数的输出不以零为中心，这可能导致后续层的权重更新方向不一致，使得网络收敛速度较慢。
```
Tanh函数：
```
梯度消失：特别是在输入值较大或较小的情况下，梯度接近于零。
非零均值：Tanh 函数的输出也不以零为中心，可能导致模型训练时的不稳定性。
```
LeRu
```
死亡ReLU: ReLU 函数在负数输入时输出为零，导致了所谓的 "死亡 ReLU" 问题。当神经元输出为零时，梯度也为零，这意味着对应神经元的权重不会得到更新，使其永远处于非活跃状态。
不适用于负值：ReLU 对负数的输入输出为零，这可能导致信息损失。在某些情况下，希望神经网络能够对所有输入都有响应，而不是将一部分输入映射为零。
不适用于序列数据：因为它对负值的抑制可能导致失去输入序列中的一些关键信息。
```
总体建议
```
- 在实践中，通常会使用 Leaky ReLU、Parametric ReLU 或者其他改进版本来缓解 ReLU 函数的问题。
- 针对梯度消失问题，可以使用 Batch Normalization、权重初始化技巧或者选择其他激活函数。
- 根据任务的性质和数据的特点，选择合适的激活函数。有时在同一个网络中采用不同的激活函数也是一种常见的策略。
```
$$
\begin{aligned}
Logistic: f(x)&=\frac{1}{1+e^{-x}}, f'(x)=f(x)(1-f(x))\\
Tanh: f(x)&=\frac{2}{1+e^{-2x}}-1\\
ReLU: f(x)&=max(0,x), f'(x)=I\\
ELU: f(x)&=max(0,x)+min(0,r(exp(x)-1))\\
SoftPlus: &ln(1+e^x)\\
\end{aligned}
$$

