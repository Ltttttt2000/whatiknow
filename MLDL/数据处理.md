**缺失值**
缺失值产生的原因：
信息暂时无法获取。如某种产品的收益等具有滞后效应。
数据因人为因素没有被记录、遗漏或丢失，这个是数据缺失的主要原因。
数据采集设备的故障、存储介质、传输媒体故障而造成数据丢失。
获取这些信息的代价太大。
有些对象的某个或某些属性是不可用的；如：未婚者的配偶姓名、儿童的固定收入状况等。
系统实时性能要求较高，即要求得到这些信息前迅速做出判断或决策。

据缺失的类型包括：随机丢失、完全随机丢失和非随机丢失；随机丢失意味着数据丢失的概率与丢失的数据本身无关，而仅与部分已观测到的数据有关。完全随机丢失指的是数据的缺失依赖于不完全变量自身。非随机丢失数据指的是数据的缺失依赖于不完全变量自身。

对于缺失值的处理方式包括：删除样本、插补和保留不处理三种方式；

**方法**
k-means是最基础的聚类算法，它的输入需要簇的个数k，聚类目标是使得类内的点足够近，类间的点足够远
KNN是机器学习中的一种分类算法，并不是聚类算法；
DBSCAN是基于密度的聚类的算法，要求聚类空间的一定区域所包含的对象的数目不小于某一给定阈值，当邻近区域的密度超过某个阈值，则继续聚类；
GCN分类本质上是基于图的聚类，然而基于GCN的聚类算法会有深度学习中的训练的概念，而传统的聚类算法则是通过人工设定阈值来决定的。

用来衡量数据离散程度的指标包括：极差、四分位差、方差、标准差、平均差和变异系数 （众数不是）

经常使用的标准化方法包括：极差标准化法（min-max方法）；Z-score标准化法；线性比例标准化法：极大化法、极小值法、log函数标准化法和反正切函数标准化法；PCA方法又名主成分分析方法，是一种数据降维方法，并不是标准化方法；
在相关性分析中需要相关的两个变量：因变量是随机的量，自变量也是随机的量
回归分析：因变量是随机的量，自变量是控制的量

使用3σ方法要保证历史数据异常点较少，因为异常点多的话均值容易被异常点拉偏，使用3σ方法的话数据就不太可靠；
3σ方法的条件是数据需要服从正态分布，在3∂原则下，异常值如超过3倍标准差，那么可以将其视为异常值，在处理数据时，应剔除高度异常的异常值。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述，再进行剔除。
DBSCAN聚类算法是一种基于密度的离群点检测，可以用来做异常值检测； 
## sklearn
关于sklearn中的train_test_split函数
数据集划分的比例可以通过test_size样本占比参数进行指定
通过train_test_split函数只能划分训练集和测试集两个集合
虽然train_test_split函数是从样本中随机的按比例选取训练数据和测试数据，但是可以通过random_state随机数的种子参数进行固定划分；
参数stratify可以处理数据类型不平衡问题，依据标签y，按原数据y中各类比例，分配给train和test，使得train和test中各类数据的比例与原数据集一致。


**处理样本不均衡问题**
```
1. 样本层面
(1) 欠拟样：随机删除样本量多的类，可能被删除的有重要信息
(2) 过拟样：拷贝现有样本随机增加少的类，可能导致过拟合
(3) 合成采样SOMTE：基于距离度量计算样本间相似性，选择一个基础样本，在邻居样本中随机选择一定数量样本对基础样本的一个属性进行噪声，每次处理一个属性
(4) 数据增强Data Augmentation：
① 单样本增强（图像）：几何、颜色、添加噪声等
② 多样本：SMOTE, SamplePairing, mixup, 上下采样等
③ 基于GAN

2. 损失函数层面
(1) class weight: scikit模型的class weight方法，为不同类别样本提供不同权重
(2) Focal Loss：在交叉熵损失函数（CE）的基础上增加了类别的不同权重以及困难（高损失）样本的权重
(3) OHEM：（Online Hard Example Mining）选择一些hard examples（多样性和高损失的样本）作为训练的样本，针对性地改善模型学习效果。对于数据的类别不平衡问题，OHEM的针对性更强。

3. 模型层面
(1) 采样+集成学习：通过重复组合少数类样本与抽样的同样数量的多数类样本，训练若干的分类器进行集成学习。
① BalanceCascade：基于Adaboost作为基分类器，在每一轮训练时都使用多数类与少数类数量上相等的训练集，使用分类器对全体多数类进行预测，通过控制分类阈值来控制FP(false positive)，将所有判断正确的类删除，进入下一轮迭代继续降低多数类数量。（数据噪声较小时用，对噪声敏感易过拟合）
② EasyEnsemble：基于Adaboost作为基分类器，将多数样本随机分成N个子集，每一个子集样本与少数类样本相同，分别将各个多数类样本子集与少数类样本进行组合，用AdaBoost进行训练, bagging （噪声大时）
(2) 异常检测：少数类只有几个时将分类考虑成异常检测anomaly detection

4. 决策及评估指标
(1) P-R曲线：分类阈值移动来调整
(2) AUC/AUPRC：对正负样本的比例情况不敏感，ROC曲线面积不会太大变化
```
# 数据集问题
1. **数据质量问题：**
    - **标签质量：** 数据集中标签的准确性和一致性对于监督学习任务至关重要。如果标签存在错误或不一致，可能会影响模型的训练和性能。
    - **异常值：** 数据集中的异常值可能对模型产生负面影响。需要对数据进行异常值检测和处理。
2. **数据量问题：**
    - **不平衡数据：** 类别不平衡可能导致模型对多数类别过于偏向，而忽略少数类别。需要采取方法来处理不平衡数据，如重采样或使用类别权重。
    - **小样本问题：** 当数据量较小时，模型容易过拟合。在小数据集上训练深度学习模型可能需要采用迁移学习、数据增强等技术。
3. **数据采集问题：**
    - **标签偏见：** 数据集的采集方式可能导致标签偏见，从而影响模型的泛化。需注意数据采集的均衡性和代表性。
    - **样本选择偏见：** 数据集中的样本选择可能不是随机的，可能导致模型在特定场景下的性能失效。
4. **数据预处理问题：**
    - **图像质量：** 图像数据集中可能存在不同质量的图像，需要进行图像预处理，确保图像质量一致。
    - **数据标准化：** 数据的尺度和范围可能不同，需要进行标准化以确保模型的稳定性。
5. **领域适应问题：**
    - **训练集与测试集差异：** 如果训练集与实际应用场景中的数据有较大差异，可能导致模型在实际场景中的性能下降。需要注意领域适应问题。
6. **隐私和伦理问题：**
    - **敏感信息：** 数据集中可能包含敏感信息，需要谨慎处理以保护用户隐私。
    - **公平性：** 数据集的构建和使用可能存在公平性问题，需要关注模型在不同子群体中的性能差异。
7. **长期可维护性问题：**
    - **数据更新：** 随着时间推移，数据的分布可能发生变化，需要考虑定期更新数据集以维护模型的性能。
8. 什么样的数据集不适合用深度学习
- 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。
- 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果

