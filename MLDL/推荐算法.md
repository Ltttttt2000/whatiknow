# TF-IDF
TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于**挖掘文章中的关键词**，而且算法简单高效，常被工业用于最开始的文本数据清洗。
**TF-IDF的主要思想是**：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

TF：词频
IDF：逆文档频率
高频词例如“的”等过滤后只剩下实际意义的词

将TF和IDF相成得到值，值越大，这个词在文章中重要性越高，由大到小排序生成文章的关键词
Step 1:
TF=某个词在文章中出现的次数
考虑不同文章的比较，进行标准化，防止偏向较长的文档。
```
TF = 某个词在文章中出现次数/文章总词数
```
Step 2:
计算IDF需要语料库模拟语言使用环境
IDF = log(语料库的文档总数/包含该词的文档数+1)
一个词越常见分母越大
Step 3:
TF-IDF = TF\*IDF
TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。
 某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。
所以，自动提取关键词的算法就很清楚了，就是**计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。**
TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用**词频**来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。如果要体现词的上下文结构，那么你可能需要使用word2vec算法来支持。
```python
# -*- coding: utf-8 -*-
from collections import defaultdict
import math
import operator
 
"""
函数说明:创建数据样本
Returns:
    dataset - 实验样本切分的词条
    classVec - 类别标签向量
"""
def loadDataSet():
    dataset = [ ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],    # 切分的词条
                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid'] ]
    classVec = [0, 1, 0, 1, 0, 1]  # 类别标签向量，1代表好，0代表不好
    return dataset, classVec
 
 
"""
函数说明：特征选择TF-IDF算法
Parameters:
     list_words:词列表
Returns:
     dict_feature_select:特征选择词字典
"""
def feature_select(list_words):
    #总词频统计
    doc_frequency=defaultdict(int)
    for word_list in list_words:
        for i in word_list:
            doc_frequency[i]+=1
 
    #计算每个词的TF值
    word_tf={}  #存储没个词的tf值
    for i in doc_frequency:
        word_tf[i]=doc_frequency[i]/sum(doc_frequency.values())
 
    #计算每个词的IDF值
    doc_num=len(list_words)
    word_idf={} #存储每个词的idf值
    word_doc=defaultdict(int) #存储包含该词的文档数
    for i in doc_frequency:
        for j in list_words:
            if i in j:
                word_doc[i]+=1
    for i in doc_frequency:
        word_idf[i]=math.log(doc_num/(word_doc[i]+1))
 
    #计算每个词的TF*IDF的值
    word_tf_idf={}
    for i in doc_frequency:
        word_tf_idf[i]=word_tf[i]*word_idf[i]
 
    # 对字典按值由大到小排序
    dict_feature_select=sorted(word_tf_idf.items(),key=operator.itemgetter(1),reverse=True)
    return dict_feature_select
 
if __name__=='__main__':
    data_list,label_list=loadDataSet() #加载数据
    features=feature_select(data_list) #所有词的TF-IDF值
    print(features)
    print(len(features))
```

应用：搜索引擎、关键词提取、文本相似性、文本摘要
NLTK
```python
from nltk.text import TextCollection
from nltk.tokenize import word_tokenize
 
#首先，构建语料库corpus
sents=['this is sentence one','this is sentence two','this is sentence three']
sents=[word_tokenize(sent) for sent in sents] #对每个句子进行分词
print(sents)  #输出分词后的结果
corpus=TextCollection(sents)  #构建语料库
print(corpus)  #输出语料库
 
#计算语料库中"one"的tf值
tf=corpus.tf('one',corpus)    # 1/12
print(tf)
 
#计算语料库中"one"的idf值
idf=corpus.idf('one')      #log(3/1)
print(idf)
 
#计算语料库中"one"的tf-idf值
tf_idf=corpus.tf_idf('one',corpus)
print(tf_idf)
```
Sklearn
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
 
x_train = ['TF-IDF 主要 思想 是','算法 一个 重要 特点 可以 脱离 语料库 背景',
           '如果 一个 网页 被 很多 其他 网页 链接 说明 网页 重要']
x_test=['原始 文本 进行 标记','主要 思想']
 
#该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频
vectorizer = CountVectorizer(max_features=10)
#该类会统计每个词语的tf-idf权值
tf_idf_transformer = TfidfTransformer()
#将文本转为词频矩阵并计算tf-idf
tf_idf = tf_idf_transformer.fit_transform(vectorizer.fit_transform(x_train))
#将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重
x_train_weight = tf_idf.toarray()
 
#对测试集进行tf-idf权重计算
tf_idf = tf_idf_transformer.transform(vectorizer.transform(x_test))
x_test_weight = tf_idf.toarray()  # 测试集TF-IDF权重矩阵
 
print('输出x_train文本向量：')
print(x_train_weight)
print('输出x_test文本向量：')
print(x_test_weight)
```

jieba分词

```python
import jieba.analyse
 
text='关键词是能够表达文档中心内容的词语，常用于计算机系统标引论文内容特征、信息检索、系统汇集以供读者检阅。关键词提取是文本挖掘领域的一个分支，是文本检索、文档比较、摘要生成、文档分类和聚类等文本挖掘研究的基础性工作'
 
keywords=jieba.analyse.extract_tags(text, topK=5, withWeight=False, allowPOS=())
print(keywords)
```

```
jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPos=())

sentence: 待提取的文本
topK：返回几个权重最大的关键词，默认值为20
withWeight: 是否一并返回关键词权重，默认值是False
allowPOS：仅包括制定词性的词，默认值为空，即不筛选
```

TF-IDF算法的不足
TF-IDF 采用文本逆频率 IDF 对 TF 值加权取权值大的作为关键词，但 IDF 的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以 TF-IDF 算法的精度并不是很高，尤其是当文本集已经分类的情况下。

在本质上 IDF 是一种试图抑制噪音的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用。这对于大部分文本信息，并不是完全正确的。IDF 的简单结构并不能使提取的关键词， 十分有效地反映单词的重要程度和特征词的分布情 况，使其无法很好地完成对权值调整的功能。尤其是在同类语料库中，这一方法有很大弊端，往往一些同类文本的关键词被盖。

TF-IDF算法实现简单快速，但是仍有许多不足之处：

（1）没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。
（2）按照传统TF-IDF，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。
（3）传统TF-IDF中的IDF部分只考虑了特征词与它出现的文本数之间的关系，而忽略了特征项在一个类别中不同的类别间的分布情况。
（4）对于文档中出现次数较少的重要人名、地名信息提取效果不佳。

TF-IDF算法改进——TF-IWF算法

词性还原

## **协同过滤推荐**
协同过滤推荐的主要思想是，利用已有用户群**过去的行为**预测当前用户最可能喜欢哪些物品。
输入：“用户-物品”评分矩阵
输出：一种是用户对某个物品喜欢或不喜欢程度的预测数值；另一种是 n 项推荐物品的列表

协同过滤推荐大致可以分为：
1. 基于邻域的推荐：将所有数据记忆到存储体中
		基于用户的协同过滤：给用户推荐和他兴趣相似的其他用户喜欢的物品  
		基于物品的协同过滤：给用户推荐和他之前喜欢的物品相似的物品

2.  基于模型的推荐。（离线）做数据降维，抽象出特征，运行时直接用特征。
使用部分机器学习算法，找出用户与项的相互作用模型，从而找出数据中的特定模式。如关联模型，隐语义模型、图模型、混聚类模型、分类模型、回归模型、矩阵分解模型、神经网络模型、合模型、深度学习模型等。

#### 基于用户的协同过滤推荐

**步骤：**
1、找到和目标用户兴趣相似的用户集合。
2、找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。

**缺点：**
1、UserCF 需要维护一个用户兴趣相似度的矩阵，随着用户数目增多，维护用户兴趣相似度矩阵的代价越大。计算用户兴趣相似度矩阵的运算时间复杂度和空间复杂度的增长和用户数的增长近似于平方关系。
2、当数据发生变化的时候，之前计算出的用户之间的相似度不稳定。
3、基于用户的协同过滤很难对推荐结果作出解释。

**实用场景 —— 新闻推荐：**

1、个性化新闻推荐更加强调抓住新闻热点，热门程度和时效性是个性化新闻推荐的重点，而个性化相对于这两点略显次要。

2、新闻的更新非常快，物品相关度的表也需要很快更新，虽然 UserCF 对于新用户也需要更新相似度表，但在新闻网站中，物品的更新速度远远快于新用户的加入速度，而且对于新用户，完全可以给他推荐最热门的新闻。


**举例：**

基于用户的协同过滤推荐简单的说，如果 A、B 两个用户都购买了 x、y、z 三本图书，并且给出了 5 星的好评。那么 A 和 B 就属于同一类用户。可以将 A 看过的图书 w 也推荐给用户 B。




推荐算法
## 基于内容推荐的原理

基于内容推荐的工作原理是，评估用户还没看到的物品与当前用户过去喜欢的物品的相似程度。
## 基于内容推荐的过程

基于内容推荐的过程一般包括以下三步：

> 1、给出物品表示：为每个物品抽取出一些特征来表示此物品；  
> 2、学习用户偏好：利用一个用户过去喜欢（及不喜欢）的物品的特征数据，来学习出此用户偏好；  
> 3、生成推荐列表：根据候选物品表示和用户偏好，为该用户生成其最可能感兴趣的 n 个物品。


举例说明（个性化阅读）：

> 第一步，对于个性化阅读来说一个 item 就是一篇文章，我们首先要从文章内容中抽取出代表它们的属性。常用的方法就是利用出现在一篇文章中的词来代表这篇文章，而每个词对应的权重往往使用信息检索中的 **tf-idf** 来计算。利用这种方法，一篇抽象的文章就可以使用具体的一个向量来表示了。  
> 第二步就是根据用户过去喜欢什么文章来产生刻画此用户喜好的 profile 了，最简单的方法可以把用户所有喜欢的文章对应的向量的平均值作为此用户的 profile。  
> 第三步，在获得了一个用户的 profile 后，CB 就可以利用所有 item 与此用户 profile 的相关度对他进行推荐文章了。一个常用的相关度计算方法是 cosine。最终把候选 item 里与此用户最相关（cosine值最大）的 N 个 item 作为推荐返回给此用户。


## 第一步：给出物品表示 - 基于向量空间模型的方法

文本表示模型有基于布尔模型、基于向量空间模型、基于主题模型、基于神经网络的方法等。
下面介绍一下基于向量空间模型的方法。

基于向量空间模型的方法是**将文本表示成实数值分量所构成的向量**，一般而言，每个分量对应一个词项，相当于将文本表示成空间中的一个点。向量不仅可以用来训练分类器，而且计算向量之间的相似度可以度量文本之间的相似度。

最常用的是 TF-IDF 计算方式，即向量的维度对应词表的大小，对应维度使用 TF-IDF 计算。

TF(term-frequence)-IDF(inverse documents frequence)，词频-反文档频率。
文本文档可以通过 TF-IDF 转换成**多维欧几里得空间中的向量**，空间的维度对应文档中出现的关键词。给定文档在每维（即每个词）的坐标由两个子量的乘积得出：词频和反文档频率。

TF，词频。描述某个词在一篇文档中出现的频繁程度。考虑到文档长度，为了阻止更长的文档得到更高的相关度权值，必须进行文档长度的某种归一化。其中一种简单方法是将词出现的实际次数比上文档中其他关键词出现的最多次数，即 TF(i,j) = freq(i,j)/maxOthers(i,j)，其中 TF(i,j) 为文档 j 中关键词 i 的归一化词频值，freq(i,j) 为 i 在 j 中出现的绝对频率，maxOthers(i,j) 为 j 中其他关键词出现的最多次数。

IDF，反文档频率。组合了词频后的第二个衡量值，旨在降低所有文档中几乎都会出现的关键词的权重。其思想是，那些常见的词语对区分文档没有用，应该给那些仅出现在某些文档中的词更高的权值。IDF的计算方式为 IDF(i) = log(N/n(i))，其中 IDF(i) 为关键词 i 的反文档频率，N 为所有可推荐文档的数量，n(i) 为 N 中关键词 i 出现过文档的数量。

文档 j 中关键词 i 的组合 TF-IDF 权值可以计算为这两个子量的乘积，即 TF-IDF(i,j) = TF(i,j) * IDF(i)。

向量空间模型的优点是简单明了，向量维度意义明确，效果不错。但也存在明显的缺点，其一是维度随着词表增大而增大，且向量高度稀疏；其二是无法处理“一义多词”和“一词多义”问题。针对以上问题，有人提出很多改进方法，如删除停用词并还原词干、通过特征选择选择可用词子集、通过矩阵分解对高维稀疏矩阵降维等。

## 第二步：学习用户偏好 - 基于最近邻模型的方法

假设某用户已经对一些物品给出了他的喜好判断。那么这一步要做的就是通过该用户过去的这些喜好判断为他产生一个模型。之后我们就可以根据此模型来判断该用户是否会喜欢一个新的物品。
所以，这类问题可以看作是一个典型的**有监督分类问题**，理论上机器学习里的分类算法都可以照搬进这里。下面介绍一下基于最近邻模型的方法。

在基于最近邻模型的方法中，对于一个新的物品 item，最近邻方法首先找用户 u 已经评判过并与此新物品 item 最相似的 k 个物品，然后依据用户 u 对这 k 个物品的喜好程度来判断其对此新物品 item 的喜好程度。对于这个方法，比较关键的可能就是如何通过物品的属性向量计算物品之间的两两相似度。如果使用向量空间模型来表示物品的话，相似度计算可以使用余弦相似度。

基于最近邻模型的方法的优点是，易于实现，快速适应变化，相对少的数据效果也不错；缺点是，预测精度低。

## 第三步：生成推荐列表

如果上一步学习用户偏好中使用的是分类模型（如朴素贝叶斯算法），那么我们只要把模型预测的用户最可能感兴趣的 n 个物品作为推荐结果返回给用户即可。

而如果上一步学习用户偏好中使用的是直接学习用户偏好的方法（如Rocchio算法），那么我们只要把与用户偏好最相关的 n 个物品作为推荐结果返回给用户即可，其中的用户偏好与物品表示的相关性可以使用相似度度量（如余弦相似度）获得。
## 基于内容推荐的优缺点

优点：
1、用户之间的独立性：既然每个用户的用户偏好都是依据他本身对物品的喜好获得的，自然就与他人的行为无关。而协同过滤推荐刚好相反，协同过滤推荐需要利用很多其他人的数据。基于内容推荐的这种用户独立性带来的一个显著好处是别人不管对物品如何作弊（比如利用多个账号把某个产品的排名刷上去）都不会影响到自己。
2、好的可解释性：如果需要向用户解释为什么推荐了这些产品给他，你只要告诉他这些产品有某某属性，这些属性跟你的品味很匹配等等。
3、新的物品可以立刻得到推荐：只要一个新的物品加进物品库，它就马上可以被推荐，被推荐的机会和老的物品是一致的。而协同过滤推荐对于新的物品就很无奈，只有当此新的物品被某些用户喜欢过（或打过分），它才可能被推荐给其他用户。所以，如果一个纯协同过滤推荐的推荐系统，新加进来的物品就永远不会被推荐。


缺点：
1、物品的特征抽取一般很难：如果系统中的物品是文档，那么我们现在可以比较容易地使用信息检索里的方法来“比较精确地”抽取出物品的特征。但很多情况下我们很难从物品中抽取出准确刻画物品的特征，比如电影推荐中物品是电影，社会化网络推荐中物品是人，这些物品属性都不好抽。其实，几乎在所有实际情况中我们抽取的物品特征都仅能代表物品的一些方面，不可能代表物品的所有方面。这样带来的一个问题就是可能从两个物品抽取出来的特征完全相同，这种情况下基于内容的方法就完全无法区分这两个物品了。
2、无法挖掘出用户的潜在兴趣：既然基于内容推荐只依赖于用户过去对某些物品的喜好，它产生的推荐也都会和用户过去喜欢的物品相似。如果一个人以前只看与推荐有关的文章，那基于内容推荐只会给他推荐更多与推荐相关的文章，它不会知道用户可能还喜欢数码。
3、无法为新用户产生推荐：新用户没有喜好历史，自然无法获得他的用户偏好，所以也就无法为他产生推荐了。当然，这个问题协同过滤推荐也有。

## 基于内容推荐的现状

基于内容推荐应该算是第一代的个性化应用中最流行的推荐算法了。但由于它本身具有某些很难解决的缺点（如上面介绍的第1点），再加上在大多数情况下其精度都不是最好的，目前大部分的推荐系统都是以其他算法为主（如协同过滤推荐），而辅以基于内容推荐以解决主算法在某些情况下的不精确性（如解决新物品问题）。但基于内容推荐的作用是不可否认的，只要具体应用中有可用的属性，那么基本都能在系统里看到基于内容推荐的影子。组合基于内容推荐和其他推荐算法的方法很多，最常用的可能是用基于内容推荐来过滤其他算法的候选集，把一些不太合适的候选（比如不要给小孩推荐偏成人的书籍）去掉。
