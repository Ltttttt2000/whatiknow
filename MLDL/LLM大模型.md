Large Language Model，是一种人工智能模型，旨在理解和生成人类语言。它们在大量的文本数据上进行训练，可以执行广泛的任务，包括文本总结、翻译、情感分析等等。它的规模很大，包含数十亿的参数。

上下文学习。GPT-3 正式引入了上下文学习能力：假设语言模型已经提供了自然语言指令和多个任务描述，它可以通过完成输入文本的词序列来生成测试实例的预期输出，而无需额外的训练或梯度更新。

指令遵循。通过对自然语言描述（即指令）格式化的多任务数据集的混合进行微调，LLM 在微小的任务上表现良好，这些任务也以指令的形式所描述。这种能力下，指令调优使 LLM 能够在不使用显式样本的情况下通过理解任务指令来执行新任务，这可以大大提高泛化能力。

循序渐进的推理。对于小语言模型，通常很难解决涉及多个推理步骤的复杂任务，例如数学学科单词问题。同时，通过思维链推理策略，LLM 可以通过利用涉及中间推理步骤的 prompt 机制来解决此类任务得出最终答案。据推测，这种能力可能是通过代码训练获得的。

训练语言模型需要向其提供大量的文本数据，模型利用这些数据来学习人类语言的结构、语法和语义。这个过程通常是通过无监督学习完成的，使用一种叫做自我监督学习的技术。在自我监督学习中，模型通过预测序列中的下一个词或标记，为输入的数据生成自己的标签，并给出之前的词。

训练过程包括两个主要步骤：预训练（pre-training）和微调（fine-tuning）：

在预训练阶段，模型从一个巨大的、多样化的数据集中学习，通常包含来自不同来源的数十亿词汇，如网站、书籍和文章。这个阶段允许模型学习一般的语言模式和表征。

在微调阶段，模型在与目标任务或领域相关的更具体、更小的数据集上进一步训练。这有助于模型微调其理解，并适应任务的特殊要求。

GPT-3（OpenAI）： Generative Pre-trained Transformer 3（GPT-3）是最著名的LLM之一，拥有1750亿个参数。该模型在文本生成、翻译和其他任务中表现出显著的性能，在全球范围内引起了热烈的反响，目前OpenAI已经迭代到了GPT-4版本

ChatGPT是由GPT-3语言模型驱动的开源聊天机器人。它能够与用户进行自然语言对话交流。



GPT（Generative Pre-trained Transformer）是一种基于 Transformer 架构的生成式自然语言处理（NLP）模型。GPT 的原理主要涉及以下三个方面：

1. 生成式（Generative）：生成式模型的目标是学习数据的概率分布，从而能够生成类似于训练数据的新数据。在 GPT 的情况下，模型学习了大量文本数据的概率分布，从而能够生成符合自然语言规律的文本。

2. 预训练（Pre-trained）：预训练指的是在模型用于特定任务之前，先在大量无标签数据上进行训练。GPT 通过学习大规模文本语料库（例如，网页、书籍等），捕捉到自然语言的语法、语义和一定程度的常识知识。这样，在应用 GPT 到具体任务时，只需进行较少量的任务相关微调，即可在各种 NLP 任务中取得良好表现。

3. 自回归（Autoregressive）：GPT 是一种自回归语言模型，这意味着它通过**预测文本序列中的下一个单词**来生成文本。在训练过程中，GPT 逐个单词地处理输入序列，并预测下一个单词的概率分布。在生成过程中，模型根据上下文和已生成的单词，逐步生成新的单词，直到完成整个文本序列。